{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training notebook for Audioset classification using attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print multiple lines per cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic model params setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(128, 1024), (1024, 1024), (1024, 1024)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_time_steps = 10  # vggish\n",
    "emb_size = 128  # vggish\n",
    "num_labels = 527  # audioset\n",
    "\n",
    "hidden_layer_sizes = [1024, 1024, 1024]\n",
    "dropout_rate = 0.5\n",
    "\n",
    "in_out_sizes = list(zip([emb_size] + hidden_layer_sizes[:-1], hidden_layer_sizes))\n",
    "in_out_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_layers(layer_size=hidden_layer_sizes[-1]):\n",
    "    def attn_pool(inputs):\n",
    "#         inputs = layers.Input(shape = input_shape)\n",
    "        feats = layers.Dense(layer_size, activation='linear')(inputs)\n",
    "        attentions = layers.Dense(layer_size, activation='sigmoid')(inputs)\n",
    "        attentions = layers.Lambda(lambda x: K.clip(x, 1e-9, 1-1e-9))(attentions)\n",
    "        attentions = attentions / K.sum(attentions, axis=1, keepdims=True)\n",
    "\n",
    "        outputs = K.sum(feats * attentions, axis = 1)\n",
    "        return outputs\n",
    "    \n",
    "    return [layers.Lambda(attn_pool)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial layers including casting int8 to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_layers = [layers.Lambda(lambda x: K.cast(x, 'float32')/128. - 1., input_shape=(num_time_steps, emb_size))]\n",
    "\n",
    "linear_layers = []\n",
    "for i,o in in_out_sizes:\n",
    "    linear_layers += [\n",
    "        layers.Dense(o, input_shape=(num_time_steps, i)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(rate=dropout_rate),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final layers after attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layers = [\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(num_labels, activation='sigmoid'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kiki\\Anaconda3\\envs\\envMgt\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\kiki\\Anaconda3\\envs\\envMgt\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda (Lambda)              (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10, 1024)          132096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 10, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10, 1024)          1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 10, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10, 1024)          1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 10, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 527)               540175    \n",
      "=================================================================\n",
      "Total params: 2,787,855\n",
      "Trainable params: 2,779,663\n",
      "Non-trainable params: 8,192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(transform_layers + linear_layers + attn_layers(1024) + final_layers)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load both bal and unbal data since batches are already balanced\n",
    "bal is about 1% of unbal data though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2063949, 10, 128)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2063949, 527)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(20371, 10, 128)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(20371, 527)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "bal_fname = 'packed_features/bal_train.h5'\n",
    "unbal_fname = 'packed_features/unbal_train.h5'\n",
    "eval_fname = 'packed_features/eval.h5'\n",
    "\n",
    "with h5py.File(bal_fname, 'r') as bal_data, h5py.File(unbal_fname, 'r') as unbal_data:\n",
    "    train_bal_x = bal_data['x'][:]\n",
    "    train_bal_y = bal_data['y'][:]\n",
    "    train_x = np.concatenate([train_bal_x, unbal_data['x'][:]])\n",
    "    train_y = np.concatenate([train_bal_y, unbal_data['y'][:]])\n",
    "\n",
    "with h5py.File(eval_fname, 'r') as eval_data:\n",
    "    test_x = eval_data['x'][:]\n",
    "    test_y = eval_data['y'][:]\n",
    "    \n",
    "train_x.shape\n",
    "train_y.shape\n",
    "test_x.shape\n",
    "test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator for balanced mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalGen(object):\n",
    "    \"\"\"Generator for mini batches with balanced number of classes. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x, y, batch_size, shuffle=True, seed=42, verbose=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x: ndarray\n",
    "          y: 2D array\n",
    "          batch_size: int\n",
    "          shuffle: bool\n",
    "          seed: int\n",
    "          verbose: int\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.rs = np.random.RandomState(seed)\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        assert self.y.ndim == 2, \"y must have dimension of 2!\"\n",
    "            \n",
    "    def get_classes_set(self, samples_num_of_classes):\n",
    "        \n",
    "        classes_num = len(samples_num_of_classes)\n",
    "        classes_set = []\n",
    "        \n",
    "        for k in range(classes_num):\n",
    "            classes_set += [k]\n",
    "            \n",
    "        return classes_set\n",
    "        \n",
    "    def generate(self, max_iteration=None):\n",
    "        \n",
    "        y = self.y\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        (samples_num, classes_num) = y.shape\n",
    "        \n",
    "        samples_num_of_classes = np.sum(y, axis=0)\n",
    "        \n",
    "        # E.g. [0, 1, 1, 2, ..., K, K]\n",
    "        classes_set = self.get_classes_set(samples_num_of_classes)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"samples_num_of_classes: {}\".format(samples_num_of_classes))\n",
    "            print(\"classes_set: {}\".format(classes_set))\n",
    "        \n",
    "        # E.g. [[0, 1, 2], [3, 4, 5, 6], [7, 8], ...]\n",
    "        indexes_of_classes = []\n",
    "        \n",
    "        for k in range(classes_num):\n",
    "            indexes_of_classes.append(np.where(y[:, k] == 1)[0])\n",
    "            \n",
    "        # Shuffle indexes\n",
    "        if self.shuffle:\n",
    "            for k in range(classes_num):\n",
    "                self.rs.shuffle(indexes_of_classes[k])\n",
    "        \n",
    "        queue = []\n",
    "        iteration = 0\n",
    "        pointers_of_classes = [0] * classes_num\n",
    "\n",
    "        while True:\n",
    "            \n",
    "            if iteration == max_iteration:\n",
    "                break\n",
    "            \n",
    "            # Get a batch containing classes from a queue\n",
    "            while len(queue) < batch_size:\n",
    "                self.rs.shuffle(classes_set)\n",
    "                queue += classes_set\n",
    "                \n",
    "            batch_classes = queue[0 : batch_size]\n",
    "            queue[0 : batch_size] = []\n",
    "            \n",
    "            samples_num_of_classes_in_batch = [batch_classes.count(k) for k in range(classes_num)]\n",
    "            batch_idxes = []\n",
    "            \n",
    "            # Get index of data from each class\n",
    "            for k in range(classes_num):\n",
    "                \n",
    "                bgn_pointer = pointers_of_classes[k]\n",
    "                fin_pointer = pointers_of_classes[k] + samples_num_of_classes_in_batch[k]\n",
    "                \n",
    "                per_class_batch_idxes = indexes_of_classes[k][bgn_pointer : fin_pointer]\n",
    "                batch_idxes.append(per_class_batch_idxes)\n",
    "\n",
    "                pointers_of_classes[k] += samples_num_of_classes_in_batch[k]\n",
    "                \n",
    "                if pointers_of_classes[k] >= samples_num_of_classes[k]:\n",
    "                    pointers_of_classes[k] = 0\n",
    "                    \n",
    "                    if self.shuffle:\n",
    "                        self.rs.shuffle(indexes_of_classes[k])\n",
    "                \n",
    "            batch_idxes = np.concatenate(batch_idxes, axis=0)\n",
    "            \n",
    "            iteration += 1\n",
    "            \n",
    "            yield self.x[batch_idxes], self.y[batch_idxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = BalGen(x, y, batch_size=256, shuffle=True, seed=42, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (batch_x, batch_y) in gen.generate(max_iteration=2):\n",
    "#     print(batch_y.sum(axis=0))\n",
    "# #     print(batch_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions for metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, roc_curve, precision_recall_curve\n",
    "\n",
    "def calc_d_prime(auc):\n",
    "    standard_normal = norm()\n",
    "    d_prime = standard_normal.ppf(auc) * np.sqrt(2.0)\n",
    "    return d_prime\n",
    "\n",
    "def calc_metrics(output, target):\n",
    "    \"\"\"Calculate all the metrics, eg. mAP, AUC etc.\n",
    "\n",
    "    Args:\n",
    "      output: 2d array, (samples_num, classes_num)\n",
    "      target: 2d array, (samples_num, classes_num)\n",
    "\n",
    "    Returns:\n",
    "      stats: list of statistic of each class.\n",
    "    \"\"\"\n",
    "\n",
    "    classes_num = target.shape[-1]\n",
    "    metrics = []\n",
    "\n",
    "    # Class-wise statistics\n",
    "    for k in range(classes_num):\n",
    "\n",
    "        # Average precision\n",
    "        avg_precision = average_precision_score(\n",
    "            target[:, k], output[:, k], average=None)\n",
    "\n",
    "        # AUC\n",
    "        auc = roc_auc_score(target[:, k], output[:, k], average=None)\n",
    "\n",
    "        # Precisions, recalls\n",
    "        (precisions, recalls, thresholds) = precision_recall_curve(\n",
    "            target[:, k], output[:, k])\n",
    "\n",
    "        # FPR, TPR\n",
    "        (fpr, tpr, thresholds) = roc_curve(target[:, k], output[:, k])\n",
    "\n",
    "        save_every_steps = 1000     # Sample statistics to reduce size\n",
    "        dict = {'precisions': precisions[0::save_every_steps],\n",
    "                'recalls': recalls[0::save_every_steps],\n",
    "                'AP': avg_precision,\n",
    "                'fpr': fpr[0::save_every_steps],\n",
    "                'fnr': 1. - tpr[0::save_every_steps],\n",
    "                'auc': auc,\n",
    "               }\n",
    "        metrics.append(dict)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def evaluate(model, x, target):\n",
    "    \"\"\"Evaluate a model on x vs target.\n",
    "\n",
    "    Args:\n",
    "      model: Keras model\n",
    "      x: 2d array, (samples_num, classes_num)\n",
    "      target: 2d array, (samples_num, classes_num)\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "\n",
    "#     callback_time = time.time()\n",
    "    (clips_num, time_steps, freq_bins) = x.shape\n",
    "    output = model.predict(x)\n",
    "\n",
    "    # Calculate statistics\n",
    "    metrics = calc_metrics(output, target)\n",
    "\n",
    "    mAP = np.mean([m['AP'] for m in metrics])\n",
    "    mAUC = np.mean([m['auc'] for m in metrics])\n",
    "    d_prime = calc_d_prime(mAUC)\n",
    "    \n",
    "    return {\n",
    "        'mAP': mAP, 'mAUC':mAUC, 'd_prime': d_prime,\n",
    "#         'mPrecisions':mPrecisions, 'mRecalls':mRecalls, 'mFpr':mFpr, 'mFnr':mFnr,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512 # so that there's roughly 1 for each class using BalGen\n",
    "print_loss_every_n_batches = 200\n",
    "eval_per_n_batches = 5000\n",
    "save_per_n_batches = 10000\n",
    "train_n_batches = 50000\n",
    "\n",
    "model_path = Path('models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_batch = 0\n",
    "bal_metrics_list = []\n",
    "test_metrics_list = []\n",
    "loss_curve_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "model_name = 'attn_feat_01_layers-2-stage_01-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kiki\\Anaconda3\\envs\\envMgt\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "loss at batch 199: 0.019132\n",
      "loss at batch 399: 0.016011\n",
      "loss at batch 599: 0.016693\n",
      "loss at batch 799: 0.015945\n",
      "loss at batch 999: 0.016737\n",
      "loss at batch 1199: 0.015309\n",
      "loss at batch 1399: 0.015688\n",
      "loss at batch 1599: 0.015097\n",
      "loss at batch 1799: 0.015388\n",
      "loss at batch 1999: 0.014993\n",
      "loss at batch 2199: 0.015038\n",
      "loss at batch 2399: 0.015414\n",
      "loss at batch 2599: 0.015405\n",
      "loss at batch 2799: 0.014987\n",
      "loss at batch 2999: 0.013950\n",
      "loss at batch 3199: 0.015175\n",
      "loss at batch 3399: 0.014958\n",
      "loss at batch 3599: 0.014311\n",
      "loss at batch 3799: 0.014534\n",
      "loss at batch 3999: 0.014268\n",
      "loss at batch 4199: 0.014787\n",
      "loss at batch 4399: 0.014120\n",
      "loss at batch 4599: 0.014587\n",
      "loss at batch 4799: 0.013869\n",
      "-------------------\n",
      "Batch: 4999, train time: 450.118 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.3778932382021831, 'mAUC': 0.9740656729295023, 'd_prime': 2.749545493363313}\n",
      "Test train metrics:\n",
      "{'mAP': 0.31393030681227535, 'mAUC': 0.9604026689788043, 'd_prime': 2.4824794619654043}\n",
      "-------------------\n",
      "\n",
      "loss at batch 4999: 0.014009\n",
      "loss at batch 5199: 0.014410\n",
      "loss at batch 5399: 0.013569\n",
      "loss at batch 5599: 0.013970\n",
      "loss at batch 5799: 0.014340\n",
      "loss at batch 5999: 0.014208\n",
      "loss at batch 6199: 0.014621\n",
      "loss at batch 6399: 0.013690\n",
      "loss at batch 6599: 0.014089\n",
      "loss at batch 6799: 0.013970\n",
      "loss at batch 6999: 0.013183\n",
      "loss at batch 7199: 0.013876\n",
      "loss at batch 7399: 0.013748\n",
      "loss at batch 7599: 0.013884\n",
      "loss at batch 7799: 0.013921\n",
      "loss at batch 7999: 0.013893\n",
      "loss at batch 8199: 0.014103\n",
      "loss at batch 8399: 0.013147\n",
      "loss at batch 8599: 0.013857\n",
      "loss at batch 8799: 0.014106\n",
      "loss at batch 8999: 0.014006\n",
      "loss at batch 9199: 0.013412\n",
      "loss at batch 9399: 0.013437\n",
      "loss at batch 9599: 0.013568\n",
      "loss at batch 9799: 0.013740\n",
      "-------------------\n",
      "Batch: 9999, train time: 438.618 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.430699933587435, 'mAUC': 0.9791136526058047, 'd_prime': 2.8790242627730693}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3288966709282552, 'mAUC': 0.9621684889145541, 'd_prime': 2.5122432230327556}\n",
      "-------------------\n",
      "\n",
      "loss at batch 9999: 0.013060\n",
      "loss at batch 10199: 0.013809\n",
      "loss at batch 10399: 0.013421\n",
      "loss at batch 10599: 0.013655\n",
      "loss at batch 10799: 0.013084\n",
      "loss at batch 10999: 0.012699\n",
      "loss at batch 11199: 0.013434\n",
      "loss at batch 11399: 0.013865\n",
      "loss at batch 11599: 0.013598\n",
      "loss at batch 11799: 0.013584\n",
      "loss at batch 11999: 0.013573\n",
      "loss at batch 12199: 0.013462\n",
      "loss at batch 12399: 0.013248\n",
      "loss at batch 12599: 0.012910\n",
      "loss at batch 12799: 0.012736\n",
      "loss at batch 12999: 0.013598\n",
      "loss at batch 13199: 0.013404\n",
      "loss at batch 13399: 0.013102\n",
      "loss at batch 13599: 0.013017\n",
      "loss at batch 13799: 0.013033\n",
      "loss at batch 13999: 0.013140\n",
      "loss at batch 14199: 0.013252\n",
      "loss at batch 14399: 0.012702\n",
      "loss at batch 14599: 0.013302\n",
      "loss at batch 14799: 0.013207\n",
      "-------------------\n",
      "Batch: 14999, train time: 440.118 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.4645896279986484, 'mAUC': 0.9818553606805352, 'd_prime': 2.960898230380087}\n",
      "Test train metrics:\n",
      "{'mAP': 0.33423928439145684, 'mAUC': 0.9624280367323864, 'd_prime': 2.516713037791232}\n",
      "-------------------\n",
      "\n",
      "loss at batch 14999: 0.013360\n",
      "loss at batch 15199: 0.013549\n",
      "loss at batch 15399: 0.013690\n",
      "loss at batch 15599: 0.012527\n",
      "loss at batch 15799: 0.013587\n",
      "loss at batch 15999: 0.013313\n",
      "loss at batch 16199: 0.013645\n",
      "loss at batch 16399: 0.013498\n",
      "loss at batch 16599: 0.014217\n",
      "loss at batch 16799: 0.013847\n",
      "loss at batch 16999: 0.013265\n",
      "loss at batch 17199: 0.013206\n",
      "loss at batch 17399: 0.012345\n",
      "loss at batch 17599: 0.013098\n",
      "loss at batch 17799: 0.013865\n",
      "loss at batch 17999: 0.012889\n",
      "loss at batch 18199: 0.013135\n",
      "loss at batch 18399: 0.013094\n",
      "loss at batch 18599: 0.012995\n",
      "loss at batch 18799: 0.012785\n",
      "loss at batch 18999: 0.013328\n",
      "loss at batch 19199: 0.012945\n",
      "loss at batch 19399: 0.012589\n",
      "loss at batch 19599: 0.013303\n",
      "loss at batch 19799: 0.012842\n",
      "-------------------\n",
      "Batch: 19999, train time: 439.371 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.48590446365995277, 'mAUC': 0.9830653511541574, 'd_prime': 3.0004299001422154}\n",
      "Test train metrics:\n",
      "{'mAP': 0.33678677751801966, 'mAUC': 0.9628679835400554, 'd_prime': 2.5243475286914414}\n",
      "-------------------\n",
      "\n",
      "loss at batch 19999: 0.012811\n",
      "loss at batch 20199: 0.012993\n",
      "loss at batch 20399: 0.012929\n",
      "loss at batch 20599: 0.013023\n",
      "loss at batch 20799: 0.012722\n",
      "loss at batch 20999: 0.012963\n",
      "loss at batch 21199: 0.013436\n",
      "loss at batch 21399: 0.012360\n",
      "loss at batch 21599: 0.013285\n",
      "loss at batch 21799: 0.013126\n",
      "loss at batch 21999: 0.012407\n",
      "loss at batch 22199: 0.012790\n",
      "loss at batch 22399: 0.012657\n",
      "loss at batch 22599: 0.012783\n",
      "loss at batch 22799: 0.012795\n",
      "loss at batch 22999: 0.012776\n",
      "loss at batch 23199: 0.012831\n",
      "loss at batch 23399: 0.012568\n",
      "loss at batch 23599: 0.013014\n",
      "loss at batch 23799: 0.012785\n",
      "loss at batch 23999: 0.012742\n",
      "loss at batch 24199: 0.012438\n",
      "loss at batch 24399: 0.012229\n",
      "loss at batch 24599: 0.012948\n",
      "loss at batch 24799: 0.013333\n",
      "-------------------\n",
      "Batch: 24999, train time: 438.884 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5003598080818955, 'mAUC': 0.9841402244191315, 'd_prime': 3.0376275643659487}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3380534898089859, 'mAUC': 0.9624578229396007, 'd_prime': 2.5172276133780835}\n",
      "-------------------\n",
      "\n",
      "loss at batch 24999: 0.013172\n",
      "loss at batch 25199: 0.012910\n",
      "loss at batch 25399: 0.012545\n",
      "loss at batch 25599: 0.012937\n",
      "loss at batch 25799: 0.011669\n",
      "loss at batch 25999: 0.012875\n",
      "loss at batch 26199: 0.013092\n",
      "loss at batch 26399: 0.012235\n",
      "loss at batch 26599: 0.012357\n",
      "loss at batch 26799: 0.012443\n",
      "loss at batch 26999: 0.012237\n",
      "loss at batch 27199: 0.013114\n",
      "loss at batch 27399: 0.012884\n",
      "loss at batch 27599: 0.012654\n",
      "loss at batch 27799: 0.012356\n",
      "loss at batch 27999: 0.013133\n",
      "loss at batch 28199: 0.012550\n",
      "loss at batch 28399: 0.013251\n",
      "loss at batch 28599: 0.013495\n",
      "loss at batch 28799: 0.012549\n",
      "loss at batch 28999: 0.012035\n",
      "loss at batch 29199: 0.013334\n",
      "loss at batch 29399: 0.012447\n",
      "loss at batch 29599: 0.012456\n",
      "loss at batch 29799: 0.013050\n",
      "-------------------\n",
      "Batch: 29999, train time: 438.711 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5139534944849845, 'mAUC': 0.9846830846869902, 'd_prime': 3.0572425669982604}\n",
      "Test train metrics:\n",
      "{'mAP': 0.33931503947229874, 'mAUC': 0.9625996092841366, 'd_prime': 2.5196816427349065}\n",
      "-------------------\n",
      "\n",
      "loss at batch 29999: 0.013270\n",
      "loss at batch 30199: 0.012680\n",
      "loss at batch 30399: 0.012807\n",
      "loss at batch 30599: 0.012754\n",
      "loss at batch 30799: 0.012547\n",
      "loss at batch 30999: 0.012578\n",
      "loss at batch 31199: 0.012264\n",
      "loss at batch 31399: 0.012592\n",
      "loss at batch 31599: 0.012903\n",
      "loss at batch 31799: 0.012250\n",
      "loss at batch 31999: 0.012379\n",
      "loss at batch 32199: 0.013090\n",
      "loss at batch 32399: 0.012579\n",
      "loss at batch 32599: 0.012265\n",
      "loss at batch 32799: 0.012019\n",
      "loss at batch 32999: 0.012968\n",
      "loss at batch 33199: 0.011979\n",
      "loss at batch 33399: 0.012554\n",
      "loss at batch 33599: 0.012825\n",
      "loss at batch 33799: 0.012692\n",
      "loss at batch 33999: 0.012458\n",
      "loss at batch 34199: 0.011981\n",
      "loss at batch 34399: 0.012542\n",
      "loss at batch 34599: 0.012123\n",
      "loss at batch 34799: 0.012065\n",
      "-------------------\n",
      "Batch: 34999, train time: 434.835 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5235646418094817, 'mAUC': 0.9852230996318592, 'd_prime': 3.077356322017252}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3395335155771609, 'mAUC': 0.9626496866697715, 'd_prime': 2.5205501949214675}\n",
      "-------------------\n",
      "\n",
      "loss at batch 34999: 0.012592\n",
      "loss at batch 35199: 0.012581\n",
      "loss at batch 35399: 0.012679\n",
      "loss at batch 35599: 0.013283\n",
      "loss at batch 35799: 0.012268\n",
      "loss at batch 35999: 0.012260\n",
      "loss at batch 36199: 0.012811\n",
      "loss at batch 36399: 0.012532\n",
      "loss at batch 36599: 0.012638\n",
      "loss at batch 36799: 0.012523\n",
      "loss at batch 36999: 0.012582\n",
      "loss at batch 37199: 0.012501\n",
      "loss at batch 37399: 0.012697\n",
      "loss at batch 37599: 0.012293\n",
      "loss at batch 37799: 0.012483\n",
      "loss at batch 37999: 0.012667\n",
      "loss at batch 38199: 0.012420\n",
      "loss at batch 38399: 0.012767\n",
      "loss at batch 38599: 0.011805\n",
      "loss at batch 38799: 0.012503\n",
      "loss at batch 38999: 0.012887\n",
      "loss at batch 39199: 0.012030\n",
      "loss at batch 39399: 0.012209\n",
      "loss at batch 39599: 0.012242\n",
      "loss at batch 39799: 0.012389\n",
      "-------------------\n",
      "Batch: 39999, train time: 429.678 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5324908891371682, 'mAUC': 0.9856715431194719, 'd_prime': 3.0945456675457335}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3410903045431127, 'mAUC': 0.9625775377998931, 'd_prime': 2.5192991320682543}\n",
      "-------------------\n",
      "\n",
      "loss at batch 39999: 0.012500\n",
      "loss at batch 40199: 0.012451\n",
      "loss at batch 40399: 0.012853\n",
      "loss at batch 40599: 0.012913\n",
      "loss at batch 40799: 0.012521\n",
      "loss at batch 40999: 0.012412\n",
      "loss at batch 41199: 0.012610\n",
      "loss at batch 41399: 0.012384\n",
      "loss at batch 41599: 0.013145\n",
      "loss at batch 41799: 0.012367\n",
      "loss at batch 41999: 0.011816\n",
      "loss at batch 42199: 0.012506\n",
      "loss at batch 42399: 0.012239\n",
      "loss at batch 42599: 0.012459\n",
      "loss at batch 42799: 0.012737\n",
      "loss at batch 42999: 0.012233\n",
      "loss at batch 43199: 0.012499\n",
      "loss at batch 43399: 0.012299\n",
      "loss at batch 43599: 0.012526\n",
      "loss at batch 43799: 0.012391\n",
      "loss at batch 43999: 0.011827\n",
      "loss at batch 44199: 0.012601\n",
      "loss at batch 44399: 0.012329\n",
      "loss at batch 44599: 0.012242\n",
      "loss at batch 44799: 0.012535\n",
      "-------------------\n",
      "Batch: 44999, train time: 429.659 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5366152762862233, 'mAUC': 0.98601353366082, 'd_prime': 3.107968735070695}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34113186373577425, 'mAUC': 0.9623490228508106, 'd_prime': 2.5153496348527957}\n",
      "-------------------\n",
      "\n",
      "loss at batch 44999: 0.012292\n",
      "loss at batch 45199: 0.012584\n",
      "loss at batch 45399: 0.012520\n",
      "loss at batch 45599: 0.012048\n",
      "loss at batch 45799: 0.012710\n",
      "loss at batch 45999: 0.012444\n",
      "loss at batch 46199: 0.012287\n",
      "loss at batch 46399: 0.013127\n",
      "loss at batch 46599: 0.012350\n",
      "loss at batch 46799: 0.011929\n",
      "loss at batch 46999: 0.012368\n",
      "loss at batch 47199: 0.012655\n",
      "loss at batch 47399: 0.012570\n",
      "loss at batch 47599: 0.012369\n",
      "loss at batch 47799: 0.012149\n",
      "loss at batch 47999: 0.012208\n",
      "loss at batch 48199: 0.012603\n",
      "loss at batch 48399: 0.012219\n",
      "loss at batch 48599: 0.012334\n",
      "loss at batch 48799: 0.012462\n",
      "loss at batch 48999: 0.012739\n",
      "loss at batch 49199: 0.012230\n",
      "loss at batch 49399: 0.012190\n",
      "loss at batch 49599: 0.012997\n",
      "loss at batch 49799: 0.012234\n",
      "-------------------\n",
      "Batch: 49999, train time: 429.476 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5434463361960243, 'mAUC': 0.986242529441006, 'd_prime': 3.117115804671608}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3410655682931599, 'mAUC': 0.9622711318777512, 'd_prime': 2.514007892339793}\n",
      "-------------------\n",
      "\n",
      "loss at batch 49999: 0.011902\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optimization method\n",
    "optimizer = Adam(lr=lr)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "DataGenerator = BalGen\n",
    "\n",
    "train_gen = DataGenerator(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "train_time = time.time()\n",
    "\n",
    "for (batch_x, batch_y) in train_gen.generate():\n",
    "\n",
    "    # Compute stats every several interations\n",
    "    if i_batch % eval_per_n_batches == eval_per_n_batches-1:\n",
    "\n",
    "        print(\"-------------------\")\n",
    "\n",
    "        print(f\"Batch: {i_batch}, train time: {time.time() - train_time:.3f} s\")\n",
    "\n",
    "        bal_metrics = evaluate(\n",
    "            model=model,\n",
    "            x=train_bal_x,\n",
    "            target=train_bal_y,\n",
    "        )\n",
    "        bal_metrics_list.append(bal_metrics)\n",
    "        print(\"Balanced train metrics:\")\n",
    "        print(bal_metrics)\n",
    "        \n",
    "        test_metrics = evaluate(\n",
    "            model=model,\n",
    "            x=test_x,\n",
    "            target=test_y,\n",
    "        )\n",
    "        test_metrics_list.append(test_metrics)\n",
    "        print(\"Test train metrics:\")\n",
    "        print(test_metrics)\n",
    "        \n",
    "        print(\"-------------------\\n\")\n",
    "        \n",
    "        train_time = time.time()\n",
    "\n",
    "    # Update params\n",
    "#     (batch_x, batch_y) = transform_data(batch_x, batch_y)\n",
    "    loss_i = model.train_on_batch(x=batch_x, y=batch_y)\n",
    "    if i_batch % print_loss_every_n_batches == print_loss_every_n_batches-1:\n",
    "        loss_curve_list.append({'batch': i_batch, 'loss': loss_i})\n",
    "        print(f'loss at batch {i_batch}: {loss_i:.6f}')\n",
    "    \n",
    "    # Save model\n",
    "    if i_batch % save_per_n_batches == save_per_n_batches-1:\n",
    "        save_model_path = model_path/(f'{model_name}after_{i_batch+1:06d}batches.h5')\n",
    "        model.save(save_model_path)\n",
    "        \n",
    "    i_batch += 1\n",
    "\n",
    "    # Stop training when maximum iteration achieves\n",
    "    if i_batch >= train_n_batches:\n",
    "#         model.save(model_path/(f'{model_name}_final.h5'))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "model_name = 'attn_feat_02_layers-2-stage_02-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n_batches += 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 50199: 0.012584\n",
      "loss at batch 50399: 0.011602\n",
      "loss at batch 50599: 0.012393\n",
      "loss at batch 50799: 0.012550\n",
      "loss at batch 50999: 0.012992\n",
      "loss at batch 51199: 0.012026\n",
      "loss at batch 51399: 0.012792\n",
      "loss at batch 51599: 0.012140\n",
      "loss at batch 51799: 0.012184\n",
      "loss at batch 51999: 0.012644\n",
      "loss at batch 52199: 0.012340\n",
      "loss at batch 52399: 0.012804\n",
      "loss at batch 52599: 0.012648\n",
      "loss at batch 52799: 0.012450\n",
      "loss at batch 52999: 0.011728\n",
      "loss at batch 53199: 0.012525\n",
      "loss at batch 53399: 0.012475\n",
      "loss at batch 53599: 0.011918\n",
      "loss at batch 53799: 0.012363\n",
      "loss at batch 53999: 0.012126\n",
      "loss at batch 54199: 0.012457\n",
      "loss at batch 54399: 0.012070\n",
      "loss at batch 54599: 0.012125\n",
      "loss at batch 54799: 0.012065\n",
      "-------------------\n",
      "Batch: 54999, train time: 446.118 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5525513251167343, 'mAUC': 0.9867198751774097, 'd_prime': 3.136613330879994}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3433285331799179, 'mAUC': 0.9626630900922429, 'd_prime': 2.520782827924879}\n",
      "-------------------\n",
      "\n",
      "loss at batch 54999: 0.012072\n",
      "loss at batch 55199: 0.012389\n",
      "loss at batch 55399: 0.011684\n",
      "loss at batch 55599: 0.012051\n",
      "loss at batch 55799: 0.012377\n",
      "loss at batch 55999: 0.012410\n",
      "loss at batch 56199: 0.012736\n",
      "loss at batch 56399: 0.011962\n",
      "loss at batch 56599: 0.012607\n",
      "loss at batch 56799: 0.012392\n",
      "loss at batch 56999: 0.011819\n",
      "loss at batch 57199: 0.012150\n",
      "loss at batch 57399: 0.012078\n",
      "loss at batch 57599: 0.011998\n",
      "loss at batch 57799: 0.012088\n",
      "loss at batch 57999: 0.011909\n",
      "loss at batch 58199: 0.012412\n",
      "loss at batch 58399: 0.011714\n",
      "loss at batch 58599: 0.012171\n",
      "loss at batch 58799: 0.012573\n",
      "loss at batch 58999: 0.012130\n",
      "loss at batch 59199: 0.012001\n",
      "loss at batch 59399: 0.011949\n",
      "loss at batch 59599: 0.011922\n",
      "loss at batch 59799: 0.012050\n",
      "-------------------\n",
      "Batch: 59999, train time: 430.693 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5563591786584243, 'mAUC': 0.9868874372751335, 'd_prime': 3.143601197248483}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3432484296722325, 'mAUC': 0.962572590948209, 'd_prime': 2.5192134257520973}\n",
      "-------------------\n",
      "\n",
      "loss at batch 59999: 0.011548\n",
      "loss at batch 60199: 0.012033\n",
      "loss at batch 60399: 0.011643\n",
      "loss at batch 60599: 0.012047\n",
      "loss at batch 60799: 0.011805\n",
      "loss at batch 60999: 0.011620\n",
      "loss at batch 61199: 0.011988\n",
      "loss at batch 61399: 0.012230\n",
      "loss at batch 61599: 0.012032\n",
      "loss at batch 61799: 0.012076\n",
      "loss at batch 61999: 0.011851\n",
      "loss at batch 62199: 0.012225\n",
      "loss at batch 62399: 0.012094\n",
      "loss at batch 62599: 0.011826\n",
      "loss at batch 62799: 0.011545\n",
      "loss at batch 62999: 0.011997\n",
      "loss at batch 63199: 0.012305\n",
      "loss at batch 63399: 0.011847\n",
      "loss at batch 63599: 0.011929\n",
      "loss at batch 63799: 0.011990\n",
      "loss at batch 63999: 0.012010\n",
      "loss at batch 64199: 0.012219\n",
      "loss at batch 64399: 0.011654\n",
      "loss at batch 64599: 0.012175\n",
      "loss at batch 64799: 0.011888\n",
      "-------------------\n",
      "Batch: 64999, train time: 430.295 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5593068402448854, 'mAUC': 0.9870536866286197, 'd_prime': 3.150611008641324}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34354973408937256, 'mAUC': 0.9625275653235527, 'd_prime': 2.5184337626272675}\n",
      "-------------------\n",
      "\n",
      "loss at batch 64999: 0.012079\n",
      "loss at batch 65199: 0.012321\n",
      "loss at batch 65399: 0.012768\n",
      "loss at batch 65599: 0.011730\n",
      "loss at batch 65799: 0.012693\n",
      "loss at batch 65999: 0.012190\n",
      "loss at batch 66199: 0.012580\n",
      "loss at batch 66399: 0.012619\n",
      "loss at batch 66599: 0.013231\n",
      "loss at batch 66799: 0.012788\n",
      "loss at batch 66999: 0.012430\n",
      "loss at batch 67199: 0.012270\n",
      "loss at batch 67399: 0.011354\n",
      "loss at batch 67599: 0.012194\n",
      "loss at batch 67799: 0.012914\n",
      "loss at batch 67999: 0.011818\n",
      "loss at batch 68199: 0.012218\n",
      "loss at batch 68399: 0.012038\n",
      "loss at batch 68599: 0.012079\n",
      "loss at batch 68799: 0.011904\n",
      "loss at batch 68999: 0.012308\n",
      "loss at batch 69199: 0.011862\n",
      "loss at batch 69399: 0.011766\n",
      "loss at batch 69599: 0.012313\n",
      "loss at batch 69799: 0.011944\n",
      "-------------------\n",
      "Batch: 69999, train time: 430.087 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5613988649934056, 'mAUC': 0.9871596547855052, 'd_prime': 3.1551198214636997}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3434030825299983, 'mAUC': 0.9624918689263184, 'd_prime': 2.5178161878117913}\n",
      "-------------------\n",
      "\n",
      "loss at batch 69999: 0.011857\n",
      "loss at batch 70199: 0.011902\n",
      "loss at batch 70399: 0.012049\n",
      "loss at batch 70599: 0.012115\n",
      "loss at batch 70799: 0.012079\n",
      "loss at batch 70999: 0.012078\n",
      "loss at batch 71199: 0.012126\n",
      "loss at batch 71399: 0.011321\n",
      "loss at batch 71599: 0.012286\n",
      "loss at batch 71799: 0.012443\n",
      "loss at batch 71999: 0.011674\n",
      "loss at batch 72199: 0.011901\n",
      "loss at batch 72399: 0.011752\n",
      "loss at batch 72599: 0.011871\n",
      "loss at batch 72799: 0.012249\n",
      "loss at batch 72999: 0.012109\n",
      "loss at batch 73199: 0.012222\n",
      "loss at batch 73399: 0.011827\n",
      "loss at batch 73599: 0.011995\n",
      "loss at batch 73799: 0.011941\n",
      "loss at batch 73999: 0.011788\n",
      "loss at batch 74199: 0.011610\n",
      "loss at batch 74399: 0.011476\n",
      "loss at batch 74599: 0.012067\n",
      "loss at batch 74799: 0.012353\n",
      "-------------------\n",
      "Batch: 74999, train time: 430.008 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5628497748474454, 'mAUC': 0.9872544050902085, 'd_prime': 3.159178666153502}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3433650932409981, 'mAUC': 0.9624768856031616, 'd_prime': 2.517557107932674}\n",
      "-------------------\n",
      "\n",
      "loss at batch 74999: 0.012228\n",
      "loss at batch 75199: 0.012050\n",
      "loss at batch 75399: 0.011863\n",
      "loss at batch 75599: 0.012249\n",
      "loss at batch 75799: 0.011141\n",
      "loss at batch 75999: 0.012380\n",
      "loss at batch 76199: 0.012398\n",
      "loss at batch 76399: 0.011481\n",
      "loss at batch 76599: 0.011714\n",
      "loss at batch 76799: 0.011895\n",
      "loss at batch 76999: 0.011588\n",
      "loss at batch 77199: 0.011860\n",
      "loss at batch 77399: 0.012223\n",
      "loss at batch 77599: 0.011885\n",
      "loss at batch 77799: 0.011709\n",
      "loss at batch 77999: 0.012241\n",
      "loss at batch 78199: 0.011784\n",
      "loss at batch 78399: 0.012560\n",
      "loss at batch 78599: 0.012593\n",
      "loss at batch 78799: 0.011803\n",
      "loss at batch 78999: 0.011393\n",
      "loss at batch 79199: 0.012821\n",
      "loss at batch 79399: 0.011792\n",
      "loss at batch 79599: 0.011786\n",
      "loss at batch 79799: 0.012302\n",
      "-------------------\n",
      "Batch: 79999, train time: 429.977 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5647317252851674, 'mAUC': 0.9872997258181377, 'd_prime': 3.1611293194504064}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34332114359041455, 'mAUC': 0.9623847868893879, 'd_prime': 2.5159664620035547}\n",
      "-------------------\n",
      "\n",
      "loss at batch 79999: 0.012123\n",
      "loss at batch 80199: 0.012056\n",
      "loss at batch 80399: 0.012131\n",
      "loss at batch 80599: 0.011939\n",
      "loss at batch 80799: 0.011761\n",
      "loss at batch 80999: 0.011950\n",
      "loss at batch 81199: 0.011607\n",
      "loss at batch 81399: 0.011820\n",
      "loss at batch 81599: 0.012343\n",
      "loss at batch 81799: 0.011688\n",
      "loss at batch 81999: 0.011679\n",
      "loss at batch 82199: 0.012197\n",
      "loss at batch 82399: 0.011976\n",
      "loss at batch 82599: 0.011703\n",
      "loss at batch 82799: 0.011264\n",
      "loss at batch 82999: 0.012036\n",
      "loss at batch 83199: 0.011552\n",
      "loss at batch 83399: 0.012013\n",
      "loss at batch 83599: 0.012352\n",
      "loss at batch 83799: 0.012096\n",
      "loss at batch 83999: 0.011883\n",
      "loss at batch 84199: 0.011618\n",
      "loss at batch 84399: 0.011998\n",
      "loss at batch 84599: 0.011542\n",
      "loss at batch 84799: 0.011553\n",
      "-------------------\n",
      "Batch: 84999, train time: 430.172 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5668125175813595, 'mAUC': 0.9873873616499034, 'd_prime': 3.1649184083374036}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3429552701839612, 'mAUC': 0.9623873449999509, 'd_prime': 2.5160106004242335}\n",
      "-------------------\n",
      "\n",
      "loss at batch 84999: 0.012016\n",
      "loss at batch 85199: 0.012395\n",
      "loss at batch 85399: 0.011835\n",
      "loss at batch 85599: 0.012305\n",
      "loss at batch 85799: 0.011684\n",
      "loss at batch 85999: 0.011654\n",
      "loss at batch 86199: 0.012408\n",
      "loss at batch 86399: 0.011803\n",
      "loss at batch 86599: 0.012223\n",
      "loss at batch 86799: 0.011889\n",
      "loss at batch 86999: 0.011969\n",
      "loss at batch 87199: 0.011986\n",
      "loss at batch 87399: 0.012316\n",
      "loss at batch 87599: 0.011800\n",
      "loss at batch 87799: 0.011828\n",
      "loss at batch 87999: 0.011799\n",
      "loss at batch 88199: 0.012239\n",
      "loss at batch 88399: 0.012282\n",
      "loss at batch 88599: 0.011601\n",
      "loss at batch 88799: 0.012025\n",
      "loss at batch 88999: 0.012358\n",
      "loss at batch 89199: 0.011284\n",
      "loss at batch 89399: 0.011720\n",
      "loss at batch 89599: 0.011609\n",
      "loss at batch 89799: 0.011859\n",
      "-------------------\n",
      "Batch: 89999, train time: 429.862 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.568178695724057, 'mAUC': 0.9874534147273852, 'd_prime': 3.167789418827683}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3430434084005381, 'mAUC': 0.9623613326533241, 'd_prime': 2.5155618896532084}\n",
      "-------------------\n",
      "\n",
      "loss at batch 89999: 0.012090\n",
      "loss at batch 90199: 0.012175\n",
      "loss at batch 90399: 0.012385\n",
      "loss at batch 90599: 0.012620\n",
      "loss at batch 90799: 0.011951\n",
      "loss at batch 90999: 0.012027\n",
      "loss at batch 91199: 0.011951\n",
      "loss at batch 91399: 0.011681\n",
      "loss at batch 91599: 0.012338\n",
      "loss at batch 91799: 0.011819\n",
      "loss at batch 91999: 0.011629\n",
      "loss at batch 92199: 0.012033\n",
      "loss at batch 92399: 0.011897\n",
      "loss at batch 92599: 0.011904\n",
      "loss at batch 92799: 0.012319\n",
      "loss at batch 92999: 0.011947\n",
      "loss at batch 93199: 0.011937\n",
      "loss at batch 93399: 0.011946\n",
      "loss at batch 93599: 0.012234\n",
      "loss at batch 93799: 0.011949\n",
      "loss at batch 93999: 0.011505\n",
      "loss at batch 94199: 0.012228\n",
      "loss at batch 94399: 0.011995\n",
      "loss at batch 94599: 0.011469\n",
      "loss at batch 94799: 0.012060\n",
      "-------------------\n",
      "Batch: 94999, train time: 430.074 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5695532776528345, 'mAUC': 0.9875210016049528, 'd_prime': 3.1707406729103598}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34299140995736666, 'mAUC': 0.9622889589600954, 'd_prime': 2.5143147802926418}\n",
      "-------------------\n",
      "\n",
      "loss at batch 94999: 0.011639\n",
      "loss at batch 95199: 0.011875\n",
      "loss at batch 95399: 0.011993\n",
      "loss at batch 95599: 0.011565\n",
      "loss at batch 95799: 0.012249\n",
      "loss at batch 95999: 0.012113\n",
      "loss at batch 96199: 0.011803\n",
      "loss at batch 96399: 0.012544\n",
      "loss at batch 96599: 0.011861\n",
      "loss at batch 96799: 0.011487\n",
      "loss at batch 96999: 0.011843\n",
      "loss at batch 97199: 0.012075\n",
      "loss at batch 97399: 0.011917\n",
      "loss at batch 97599: 0.011697\n",
      "loss at batch 97799: 0.011821\n",
      "loss at batch 97999: 0.011594\n",
      "loss at batch 98199: 0.012091\n",
      "loss at batch 98399: 0.011941\n",
      "loss at batch 98599: 0.012018\n",
      "loss at batch 98799: 0.012107\n",
      "loss at batch 98999: 0.012366\n",
      "loss at batch 99199: 0.011776\n",
      "loss at batch 99399: 0.011529\n",
      "loss at batch 99599: 0.012424\n",
      "loss at batch 99799: 0.011734\n",
      "-------------------\n",
      "Batch: 99999, train time: 430.552 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5706129214266731, 'mAUC': 0.9875608370599358, 'd_prime': 3.1724866201056834}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34272970791967544, 'mAUC': 0.9623535847215317, 'd_prime': 2.5154282874235445}\n",
      "-------------------\n",
      "\n",
      "loss at batch 99999: 0.011731\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optimization method\n",
    "optimizer = Adam(lr=lr)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "DataGenerator = BalGen\n",
    "\n",
    "train_gen = DataGenerator(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "train_time = time.time()\n",
    "\n",
    "for (batch_x, batch_y) in train_gen.generate():\n",
    "\n",
    "    # Compute stats every several interations\n",
    "    if i_batch % eval_per_n_batches == eval_per_n_batches-1:\n",
    "\n",
    "        print(\"-------------------\")\n",
    "\n",
    "        print(f\"Batch: {i_batch}, train time: {time.time() - train_time:.3f} s\")\n",
    "\n",
    "        bal_metrics = evaluate(\n",
    "            model=model,\n",
    "            x=train_bal_x,\n",
    "            target=train_bal_y,\n",
    "        )\n",
    "        bal_metrics_list.append(bal_metrics)\n",
    "        print(\"Balanced train metrics:\")\n",
    "        print(bal_metrics)\n",
    "        \n",
    "        test_metrics = evaluate(\n",
    "            model=model,\n",
    "            x=test_x,\n",
    "            target=test_y,\n",
    "        )\n",
    "        test_metrics_list.append(test_metrics)\n",
    "        print(\"Test train metrics:\")\n",
    "        print(test_metrics)\n",
    "        \n",
    "        print(\"-------------------\\n\")\n",
    "        \n",
    "        train_time = time.time()\n",
    "\n",
    "    # Update params\n",
    "#     (batch_x, batch_y) = transform_data(batch_x, batch_y)\n",
    "    loss_i = model.train_on_batch(x=batch_x, y=batch_y)\n",
    "    if i_batch % print_loss_every_n_batches == print_loss_every_n_batches-1:\n",
    "        loss_curve_list.append({'batch': i_batch, 'loss': loss_i})\n",
    "        print(f'loss at batch {i_batch}: {loss_i:.6f}')\n",
    "    \n",
    "    # Save model\n",
    "    if i_batch % save_per_n_batches == save_per_n_batches-1:\n",
    "        save_model_path = model_path/(f'{model_name}after_{i_batch+1:06d}batches.h5')\n",
    "        model.save(save_model_path)\n",
    "        \n",
    "    i_batch += 1\n",
    "\n",
    "    # Stop training when maximum iteration achieves\n",
    "    if i_batch >= train_n_batches:\n",
    "#         model.save(model_path/(f'{model_name}_final.h5'))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "model_name = 'attn_feat_02_layers-2-stage_03-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n_batches += 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 100199: 0.012043\n",
      "loss at batch 100399: 0.011357\n",
      "loss at batch 100599: 0.012114\n",
      "loss at batch 100799: 0.012114\n",
      "loss at batch 100999: 0.012862\n",
      "loss at batch 101199: 0.012100\n",
      "loss at batch 101399: 0.012252\n",
      "loss at batch 101599: 0.011919\n",
      "loss at batch 101799: 0.012095\n",
      "loss at batch 101999: 0.012285\n",
      "loss at batch 102199: 0.012086\n",
      "loss at batch 102399: 0.012406\n",
      "loss at batch 102599: 0.012596\n",
      "loss at batch 102799: 0.012062\n",
      "loss at batch 102999: 0.011415\n",
      "loss at batch 103199: 0.012281\n",
      "loss at batch 103399: 0.012099\n",
      "loss at batch 103599: 0.011543\n",
      "loss at batch 103799: 0.012230\n",
      "loss at batch 103999: 0.011752\n",
      "loss at batch 104199: 0.012210\n",
      "loss at batch 104399: 0.011625\n",
      "loss at batch 104599: 0.012138\n",
      "loss at batch 104799: 0.011753\n",
      "-------------------\n",
      "Batch: 104999, train time: 443.702 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5710916350065195, 'mAUC': 0.9875850408770099, 'd_prime': 3.1735498148154586}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34285755664506, 'mAUC': 0.9623420122546424, 'd_prime': 2.5152287782413185}\n",
      "-------------------\n",
      "\n",
      "loss at batch 104999: 0.011846\n",
      "loss at batch 105199: 0.011953\n",
      "loss at batch 105399: 0.011287\n",
      "loss at batch 105599: 0.011595\n",
      "loss at batch 105799: 0.012228\n",
      "loss at batch 105999: 0.012104\n",
      "loss at batch 106199: 0.012605\n",
      "loss at batch 106399: 0.011675\n",
      "loss at batch 106599: 0.012186\n",
      "loss at batch 106799: 0.012016\n",
      "loss at batch 106999: 0.011612\n",
      "loss at batch 107199: 0.011985\n",
      "loss at batch 107399: 0.011576\n",
      "loss at batch 107599: 0.012204\n",
      "loss at batch 107799: 0.012166\n",
      "loss at batch 107999: 0.011561\n",
      "loss at batch 108199: 0.012190\n",
      "loss at batch 108399: 0.011637\n",
      "loss at batch 108599: 0.012073\n",
      "loss at batch 108799: 0.012352\n",
      "loss at batch 108999: 0.012087\n",
      "loss at batch 109199: 0.011901\n",
      "loss at batch 109399: 0.011870\n",
      "loss at batch 109599: 0.011722\n",
      "loss at batch 109799: 0.012158\n",
      "-------------------\n",
      "Batch: 109999, train time: 429.505 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5714188950837786, 'mAUC': 0.987601802748118, 'd_prime': 3.17428716169847}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34292727038880083, 'mAUC': 0.9622994640295401, 'd_prime': 2.5144956774142275}\n",
      "-------------------\n",
      "\n",
      "loss at batch 109999: 0.011277\n",
      "loss at batch 110199: 0.011888\n",
      "loss at batch 110399: 0.011593\n",
      "loss at batch 110599: 0.011804\n",
      "loss at batch 110799: 0.011593\n",
      "loss at batch 110999: 0.011372\n",
      "loss at batch 111199: 0.011725\n",
      "loss at batch 111399: 0.012012\n",
      "loss at batch 111599: 0.011842\n",
      "loss at batch 111799: 0.011807\n",
      "loss at batch 111999: 0.011846\n",
      "loss at batch 112199: 0.011928\n",
      "loss at batch 112399: 0.011792\n",
      "loss at batch 112599: 0.011442\n",
      "loss at batch 112799: 0.011482\n",
      "loss at batch 112999: 0.012022\n",
      "loss at batch 113199: 0.012092\n",
      "loss at batch 113399: 0.011894\n",
      "loss at batch 113599: 0.011785\n",
      "loss at batch 113799: 0.011998\n",
      "loss at batch 113999: 0.011930\n",
      "loss at batch 114199: 0.012008\n",
      "loss at batch 114399: 0.011566\n",
      "loss at batch 114599: 0.012145\n",
      "loss at batch 114799: 0.011935\n",
      "-------------------\n",
      "Batch: 114999, train time: 429.746 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5715593730323745, 'mAUC': 0.9876101790655528, 'd_prime': 3.1746559555803118}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34288215774565756, 'mAUC': 0.9623629579534287, 'd_prime': 2.5155899185255572}\n",
      "-------------------\n",
      "\n",
      "loss at batch 114999: 0.011812\n",
      "loss at batch 115199: 0.012429\n",
      "loss at batch 115399: 0.012593\n",
      "loss at batch 115599: 0.011560\n",
      "loss at batch 115799: 0.012407\n",
      "loss at batch 115999: 0.012136\n",
      "loss at batch 116199: 0.012383\n",
      "loss at batch 116399: 0.012252\n",
      "loss at batch 116599: 0.013076\n",
      "loss at batch 116799: 0.012754\n",
      "loss at batch 116999: 0.012336\n",
      "loss at batch 117199: 0.012167\n",
      "loss at batch 117399: 0.011162\n",
      "loss at batch 117599: 0.011956\n",
      "loss at batch 117799: 0.012794\n",
      "loss at batch 117999: 0.011564\n",
      "loss at batch 118199: 0.011829\n",
      "loss at batch 118399: 0.012110\n",
      "loss at batch 118599: 0.012175\n",
      "loss at batch 118799: 0.011627\n",
      "loss at batch 118999: 0.012082\n",
      "loss at batch 119199: 0.011915\n",
      "loss at batch 119399: 0.011696\n",
      "loss at batch 119599: 0.012207\n",
      "loss at batch 119799: 0.011892\n",
      "-------------------\n",
      "Batch: 119999, train time: 429.352 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5719538131232924, 'mAUC': 0.9876267042128924, 'd_prime': 3.1753841612149833}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34301417320004035, 'mAUC': 0.9623506315001327, 'd_prime': 2.5153773691664543}\n",
      "-------------------\n",
      "\n",
      "loss at batch 119999: 0.011722\n",
      "loss at batch 120199: 0.011878\n",
      "loss at batch 120399: 0.011919\n",
      "loss at batch 120599: 0.011931\n",
      "loss at batch 120799: 0.011882\n",
      "loss at batch 120999: 0.012010\n",
      "loss at batch 121199: 0.012248\n",
      "loss at batch 121399: 0.011376\n",
      "loss at batch 121599: 0.012150\n",
      "loss at batch 121799: 0.012075\n",
      "loss at batch 121999: 0.011439\n",
      "loss at batch 122199: 0.011764\n",
      "loss at batch 122399: 0.011675\n",
      "loss at batch 122599: 0.011595\n",
      "loss at batch 122799: 0.012131\n",
      "loss at batch 122999: 0.011749\n",
      "loss at batch 123199: 0.011989\n",
      "loss at batch 123399: 0.011707\n",
      "loss at batch 123599: 0.012131\n",
      "loss at batch 123799: 0.011935\n",
      "loss at batch 123999: 0.011799\n",
      "loss at batch 124199: 0.011469\n",
      "loss at batch 124399: 0.011447\n",
      "loss at batch 124599: 0.011906\n",
      "loss at batch 124799: 0.012472\n",
      "-------------------\n",
      "Batch: 124999, train time: 429.475 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.572012130654608, 'mAUC': 0.9876323260729423, 'd_prime': 3.1756320890362084}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34298159857033395, 'mAUC': 0.962310841741414, 'd_prime': 2.514691647845444}\n",
      "-------------------\n",
      "\n",
      "loss at batch 124999: 0.012066\n",
      "loss at batch 125199: 0.012015\n",
      "loss at batch 125399: 0.011602\n",
      "loss at batch 125599: 0.012078\n",
      "loss at batch 125799: 0.011032\n",
      "loss at batch 125999: 0.012054\n",
      "loss at batch 126199: 0.012251\n",
      "loss at batch 126399: 0.011287\n",
      "loss at batch 126599: 0.011638\n",
      "loss at batch 126799: 0.011781\n",
      "loss at batch 126999: 0.011519\n",
      "loss at batch 127199: 0.012006\n",
      "loss at batch 127399: 0.012122\n",
      "loss at batch 127599: 0.011822\n",
      "loss at batch 127799: 0.011755\n",
      "loss at batch 127999: 0.012037\n",
      "loss at batch 128199: 0.011620\n",
      "loss at batch 128399: 0.012507\n",
      "loss at batch 128599: 0.012412\n",
      "loss at batch 128799: 0.011852\n",
      "loss at batch 128999: 0.011103\n",
      "loss at batch 129199: 0.012602\n",
      "loss at batch 129399: 0.011579\n",
      "loss at batch 129599: 0.011587\n",
      "loss at batch 129799: 0.012464\n",
      "-------------------\n",
      "Batch: 129999, train time: 429.350 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5721017609300009, 'mAUC': 0.9876362072525687, 'd_prime': 3.175803308642788}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34295841604425303, 'mAUC': 0.9622572341026341, 'd_prime': 2.5137687283434147}\n",
      "-------------------\n",
      "\n",
      "loss at batch 129999: 0.012333\n",
      "loss at batch 130199: 0.011958\n",
      "loss at batch 130399: 0.012270\n",
      "loss at batch 130599: 0.011891\n",
      "loss at batch 130799: 0.011630\n",
      "loss at batch 130999: 0.011762\n",
      "loss at batch 131199: 0.011694\n",
      "loss at batch 131399: 0.011904\n",
      "loss at batch 131599: 0.012155\n",
      "loss at batch 131799: 0.011641\n",
      "loss at batch 131999: 0.011532\n",
      "loss at batch 132199: 0.012202\n",
      "loss at batch 132399: 0.011768\n",
      "loss at batch 132599: 0.011475\n",
      "loss at batch 132799: 0.011202\n",
      "loss at batch 132999: 0.012004\n",
      "loss at batch 133199: 0.011415\n",
      "loss at batch 133399: 0.012039\n",
      "loss at batch 133599: 0.012010\n",
      "loss at batch 133799: 0.011996\n",
      "loss at batch 133999: 0.011837\n",
      "loss at batch 134199: 0.011647\n",
      "loss at batch 134399: 0.011889\n",
      "loss at batch 134599: 0.011363\n",
      "loss at batch 134799: 0.011611\n",
      "-------------------\n",
      "Batch: 134999, train time: 430.001 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5723808192915407, 'mAUC': 0.9876494393062119, 'd_prime': 3.1763873955049973}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3429484534977048, 'mAUC': 0.9622930460068508, 'd_prime': 2.514385154280717}\n",
      "-------------------\n",
      "\n",
      "loss at batch 134999: 0.011887\n",
      "loss at batch 135199: 0.012265\n",
      "loss at batch 135399: 0.011980\n",
      "loss at batch 135599: 0.012292\n",
      "loss at batch 135799: 0.011936\n",
      "loss at batch 135999: 0.011616\n",
      "loss at batch 136199: 0.012425\n",
      "loss at batch 136399: 0.011869\n",
      "loss at batch 136599: 0.012138\n",
      "loss at batch 136799: 0.011912\n",
      "loss at batch 136999: 0.011995\n",
      "loss at batch 137199: 0.012094\n",
      "loss at batch 137399: 0.012429\n",
      "loss at batch 137599: 0.011576\n",
      "loss at batch 137799: 0.011786\n",
      "loss at batch 137999: 0.011701\n",
      "loss at batch 138199: 0.012001\n",
      "loss at batch 138399: 0.012345\n",
      "loss at batch 138599: 0.011369\n",
      "loss at batch 138799: 0.012152\n",
      "loss at batch 138999: 0.012330\n",
      "loss at batch 139199: 0.011317\n",
      "loss at batch 139399: 0.011418\n",
      "loss at batch 139599: 0.011636\n",
      "loss at batch 139799: 0.011935\n",
      "-------------------\n",
      "Batch: 139999, train time: 429.897 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5726005173140787, 'mAUC': 0.9876588071341601, 'd_prime': 3.1768012362521856}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34301762225571697, 'mAUC': 0.9623360899169143, 'd_prime': 2.5151266965749954}\n",
      "-------------------\n",
      "\n",
      "loss at batch 139999: 0.012090\n",
      "loss at batch 140199: 0.012204\n",
      "loss at batch 140399: 0.012303\n",
      "loss at batch 140599: 0.012346\n",
      "loss at batch 140799: 0.011655\n",
      "loss at batch 140999: 0.011995\n",
      "loss at batch 141199: 0.011981\n",
      "loss at batch 141399: 0.011828\n",
      "loss at batch 141599: 0.012451\n",
      "loss at batch 141799: 0.011686\n",
      "loss at batch 141999: 0.011553\n",
      "loss at batch 142199: 0.011983\n",
      "loss at batch 142399: 0.011848\n",
      "loss at batch 142599: 0.011911\n",
      "loss at batch 142799: 0.012267\n",
      "loss at batch 142999: 0.011680\n",
      "loss at batch 143199: 0.011795\n",
      "loss at batch 143399: 0.011643\n",
      "loss at batch 143599: 0.012246\n",
      "loss at batch 143799: 0.011905\n",
      "loss at batch 143999: 0.011485\n",
      "loss at batch 144199: 0.012038\n",
      "loss at batch 144399: 0.011751\n",
      "loss at batch 144599: 0.011572\n",
      "loss at batch 144799: 0.011917\n",
      "-------------------\n",
      "Batch: 144999, train time: 429.679 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5727045144549803, 'mAUC': 0.9876566965822254, 'd_prime': 3.1767079750602036}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34301599873109595, 'mAUC': 0.96226348021218, 'd_prime': 2.5138762074821313}\n",
      "-------------------\n",
      "\n",
      "loss at batch 144999: 0.011614\n",
      "loss at batch 145199: 0.011899\n",
      "loss at batch 145399: 0.011979\n",
      "loss at batch 145599: 0.011596\n",
      "loss at batch 145799: 0.012369\n",
      "loss at batch 145999: 0.011930\n",
      "loss at batch 146199: 0.011776\n",
      "loss at batch 146399: 0.012469\n",
      "loss at batch 146599: 0.011763\n",
      "loss at batch 146799: 0.011429\n",
      "loss at batch 146999: 0.011733\n",
      "loss at batch 147199: 0.012000\n",
      "loss at batch 147399: 0.011893\n",
      "loss at batch 147599: 0.012033\n",
      "loss at batch 147799: 0.011696\n",
      "loss at batch 147999: 0.011445\n",
      "loss at batch 148199: 0.012238\n",
      "loss at batch 148399: 0.011695\n",
      "loss at batch 148599: 0.011973\n",
      "loss at batch 148799: 0.012164\n",
      "loss at batch 148999: 0.012190\n",
      "loss at batch 149199: 0.011816\n",
      "loss at batch 149399: 0.011634\n",
      "loss at batch 149599: 0.012630\n",
      "loss at batch 149799: 0.011614\n",
      "-------------------\n",
      "Batch: 149999, train time: 429.511 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5729164455223863, 'mAUC': 0.987667892937315, 'd_prime': 3.177202878085732}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34313710868594, 'mAUC': 0.9622700912752858, 'd_prime': 2.5139899823324376}\n",
      "-------------------\n",
      "\n",
      "loss at batch 149999: 0.011851\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optimization method\n",
    "optimizer = Adam(lr=lr)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "DataGenerator = BalGen\n",
    "\n",
    "train_gen = DataGenerator(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "train_time = time.time()\n",
    "\n",
    "for (batch_x, batch_y) in train_gen.generate():\n",
    "\n",
    "    # Compute stats every several interations\n",
    "    if i_batch % eval_per_n_batches == eval_per_n_batches-1:\n",
    "\n",
    "        print(\"-------------------\")\n",
    "\n",
    "        print(f\"Batch: {i_batch}, train time: {time.time() - train_time:.3f} s\")\n",
    "\n",
    "        bal_metrics = evaluate(\n",
    "            model=model,\n",
    "            x=train_bal_x,\n",
    "            target=train_bal_y,\n",
    "        )\n",
    "        bal_metrics_list.append(bal_metrics)\n",
    "        print(\"Balanced train metrics:\")\n",
    "        print(bal_metrics)\n",
    "        \n",
    "        test_metrics = evaluate(\n",
    "            model=model,\n",
    "            x=test_x,\n",
    "            target=test_y,\n",
    "        )\n",
    "        test_metrics_list.append(test_metrics)\n",
    "        print(\"Test train metrics:\")\n",
    "        print(test_metrics)\n",
    "        \n",
    "        print(\"-------------------\\n\")\n",
    "        \n",
    "        train_time = time.time()\n",
    "\n",
    "    # Update params\n",
    "#     (batch_x, batch_y) = transform_data(batch_x, batch_y)\n",
    "    loss_i = model.train_on_batch(x=batch_x, y=batch_y)\n",
    "    if i_batch % print_loss_every_n_batches == print_loss_every_n_batches-1:\n",
    "        loss_curve_list.append({'batch': i_batch, 'loss': loss_i})\n",
    "        print(f'loss at batch {i_batch}: {loss_i:.6f}')\n",
    "    \n",
    "    # Save model\n",
    "    if i_batch % save_per_n_batches == save_per_n_batches-1:\n",
    "        save_model_path = model_path/(f'{model_name}after_{i_batch+1:06d}batches.h5')\n",
    "        model.save(save_model_path)\n",
    "        \n",
    "    i_batch += 1\n",
    "\n",
    "    # Stop training when maximum iteration achieves\n",
    "    if i_batch >= train_n_batches:\n",
    "#         model.save(model_path/(f'{model_name}_final.h5'))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_curve_bal = pd.DataFrame(bal_metrics_list)\n",
    "training_curve_test = pd.DataFrame(test_metrics_list)\n",
    "loss_curve = pd.DataFrame(loss_curve_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1943e02be10>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_curve.plot.line('batch', 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "    return binary_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 4 with Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "model_name = 'attn_feat_02_layers-2-stage_04-focal_loss-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n_batches += 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 150199: 476.854370\n",
      "loss at batch 150399: 391.560638\n",
      "loss at batch 150599: 409.104584\n",
      "loss at batch 150799: 376.542389\n",
      "loss at batch 150999: 377.830261\n",
      "loss at batch 151199: 338.652618\n",
      "loss at batch 151399: 366.698029\n",
      "loss at batch 151599: 342.246155\n",
      "loss at batch 151799: 351.518738\n",
      "loss at batch 151999: 350.821320\n",
      "loss at batch 152199: 344.926392\n",
      "loss at batch 152399: 357.094116\n",
      "loss at batch 152599: 353.672119\n",
      "loss at batch 152799: 339.548859\n",
      "loss at batch 152999: 326.321075\n",
      "loss at batch 153199: 347.509369\n",
      "loss at batch 153399: 339.756805\n",
      "loss at batch 153599: 320.749359\n",
      "loss at batch 153799: 343.478638\n",
      "loss at batch 153999: 337.853210\n",
      "loss at batch 154199: 346.861511\n",
      "loss at batch 154399: 328.485809\n",
      "loss at batch 154599: 337.675171\n",
      "loss at batch 154799: 317.925842\n",
      "-------------------\n",
      "Batch: 154999, train time: 443.311 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5721529672550815, 'mAUC': 0.9874276558858797, 'd_prime': 3.16666825274212}\n",
      "Test train metrics:\n",
      "{'mAP': 0.33067609367296685, 'mAUC': 0.9598414612693095, 'd_prime': 2.4732463568876226}\n",
      "-------------------\n",
      "\n",
      "loss at batch 154999: 330.449799\n",
      "loss at batch 155199: 336.219910\n",
      "loss at batch 155399: 318.525452\n",
      "loss at batch 155599: 332.064880\n",
      "loss at batch 155799: 333.877869\n",
      "loss at batch 155999: 333.316376\n",
      "loss at batch 156199: 340.049072\n",
      "loss at batch 156399: 322.799744\n",
      "loss at batch 156599: 342.872375\n",
      "loss at batch 156799: 332.389099\n",
      "loss at batch 156999: 316.825439\n",
      "loss at batch 157199: 325.815216\n",
      "loss at batch 157399: 320.408142\n",
      "loss at batch 157599: 327.051544\n",
      "loss at batch 157799: 332.708679\n",
      "loss at batch 157999: 320.342224\n",
      "loss at batch 158199: 334.772034\n",
      "loss at batch 158399: 309.670898\n",
      "loss at batch 158599: 327.483398\n",
      "loss at batch 158799: 341.377991\n",
      "loss at batch 158999: 326.347717\n",
      "loss at batch 159199: 316.425446\n",
      "loss at batch 159399: 320.547546\n",
      "loss at batch 159599: 313.948364\n",
      "loss at batch 159799: 330.012360\n",
      "-------------------\n",
      "Batch: 159999, train time: 429.497 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5775390900547501, 'mAUC': 0.9878078317646396, 'd_prime': 3.183421543966471}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3362047222268398, 'mAUC': 0.9606637604264862, 'd_prime': 2.4868113020673013}\n",
      "-------------------\n",
      "\n",
      "loss at batch 159999: 311.643738\n",
      "loss at batch 160199: 323.689758\n",
      "loss at batch 160399: 318.491821\n",
      "loss at batch 160599: 321.324219\n",
      "loss at batch 160799: 316.535645\n",
      "loss at batch 160999: 311.745972\n",
      "loss at batch 161199: 326.029602\n",
      "loss at batch 161399: 329.312866\n",
      "loss at batch 161599: 325.992462\n",
      "loss at batch 161799: 316.495056\n",
      "loss at batch 161999: 320.011017\n",
      "loss at batch 162199: 329.742676\n",
      "loss at batch 162399: 320.650543\n",
      "loss at batch 162599: 310.830383\n",
      "loss at batch 162799: 310.176971\n",
      "loss at batch 162999: 331.623169\n",
      "loss at batch 163199: 330.727112\n",
      "loss at batch 163399: 321.115204\n",
      "loss at batch 163599: 312.350525\n",
      "loss at batch 163799: 322.297607\n",
      "loss at batch 163999: 319.638641\n",
      "loss at batch 164199: 324.010620\n",
      "loss at batch 164399: 313.296783\n",
      "loss at batch 164599: 323.663208\n",
      "loss at batch 164799: 321.908386\n",
      "-------------------\n",
      "Batch: 164999, train time: 429.790 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5800514142558559, 'mAUC': 0.9879849334433223, 'd_prime': 3.1913809722883473}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3386254347629849, 'mAUC': 0.9610773890509775, 'd_prime': 2.49372206694655}\n",
      "-------------------\n",
      "\n",
      "loss at batch 164999: 325.539734\n",
      "loss at batch 165199: 327.032715\n",
      "loss at batch 165399: 338.979309\n",
      "loss at batch 165599: 311.216888\n",
      "loss at batch 165799: 334.386353\n",
      "loss at batch 165999: 331.528259\n",
      "loss at batch 166199: 337.194946\n",
      "loss at batch 166399: 330.663910\n",
      "loss at batch 166599: 346.253662\n",
      "loss at batch 166799: 347.054749\n",
      "loss at batch 166999: 335.581848\n",
      "loss at batch 167199: 330.549774\n",
      "loss at batch 167399: 303.202759\n",
      "loss at batch 167599: 322.920624\n",
      "loss at batch 167799: 340.486572\n",
      "loss at batch 167999: 317.399109\n",
      "loss at batch 168199: 318.153595\n",
      "loss at batch 168399: 326.395508\n",
      "loss at batch 168599: 322.536682\n",
      "loss at batch 168799: 319.062073\n",
      "loss at batch 168999: 320.685547\n",
      "loss at batch 169199: 319.612610\n",
      "loss at batch 169399: 312.334747\n",
      "loss at batch 169599: 325.971375\n",
      "loss at batch 169799: 316.548309\n",
      "-------------------\n",
      "Batch: 169999, train time: 430.300 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5807848205440977, 'mAUC': 0.9880866231391785, 'd_prime': 3.1959972724610215}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3393357149409337, 'mAUC': 0.9612864541110198, 'd_prime': 2.49723783088963}\n",
      "-------------------\n",
      "\n",
      "loss at batch 169999: 312.061462\n",
      "loss at batch 170199: 317.923553\n",
      "loss at batch 170399: 316.986816\n",
      "loss at batch 170599: 322.334167\n",
      "loss at batch 170799: 314.015625\n",
      "loss at batch 170999: 328.335205\n",
      "loss at batch 171199: 326.060852\n",
      "loss at batch 171399: 302.437195\n",
      "loss at batch 171599: 328.314392\n",
      "loss at batch 171799: 329.631683\n",
      "loss at batch 171999: 308.268890\n",
      "loss at batch 172199: 316.022247\n",
      "loss at batch 172399: 314.229126\n",
      "loss at batch 172599: 315.063354\n",
      "loss at batch 172799: 325.994812\n",
      "loss at batch 172999: 317.296631\n",
      "loss at batch 173199: 326.160553\n",
      "loss at batch 173399: 314.258118\n",
      "loss at batch 173599: 319.996277\n",
      "loss at batch 173799: 323.259003\n",
      "loss at batch 173999: 314.095642\n",
      "loss at batch 174199: 308.154205\n",
      "loss at batch 174399: 308.699158\n",
      "loss at batch 174599: 317.915527\n",
      "loss at batch 174799: 326.291351\n",
      "-------------------\n",
      "Batch: 174999, train time: 429.975 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5814964383604193, 'mAUC': 0.9881631642657903, 'd_prime': 3.199494528621108}\n",
      "Test train metrics:\n",
      "{'mAP': 0.340324369501798, 'mAUC': 0.9615112992347105, 'd_prime': 2.5010362673719}\n",
      "-------------------\n",
      "\n",
      "loss at batch 174999: 326.510925\n",
      "loss at batch 175199: 325.731934\n",
      "loss at batch 175399: 316.143738\n",
      "loss at batch 175599: 326.917999\n",
      "loss at batch 175799: 298.591675\n",
      "loss at batch 175999: 320.519440\n",
      "loss at batch 176199: 328.693848\n",
      "loss at batch 176399: 302.010162\n",
      "loss at batch 176599: 308.861053\n",
      "loss at batch 176799: 311.542175\n",
      "loss at batch 176999: 305.195984\n",
      "loss at batch 177199: 326.554504\n",
      "loss at batch 177399: 331.730804\n",
      "loss at batch 177599: 317.885254\n",
      "loss at batch 177799: 310.909363\n",
      "loss at batch 177999: 323.801147\n",
      "loss at batch 178199: 312.116516\n",
      "loss at batch 178399: 335.552307\n",
      "loss at batch 178599: 337.876404\n",
      "loss at batch 178799: 310.012695\n",
      "loss at batch 178999: 297.893677\n",
      "loss at batch 179199: 339.522095\n",
      "loss at batch 179399: 313.710815\n",
      "loss at batch 179599: 312.501221\n",
      "loss at batch 179799: 323.720795\n",
      "-------------------\n",
      "Batch: 179999, train time: 429.546 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5818658393575724, 'mAUC': 0.9881954025008987, 'd_prime': 3.200973407361878}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34061145164199796, 'mAUC': 0.9615725739677993, 'd_prime': 2.502074551253744}\n",
      "-------------------\n",
      "\n",
      "loss at batch 179999: 333.591248\n",
      "loss at batch 180199: 323.667358\n",
      "loss at batch 180399: 329.278259\n",
      "loss at batch 180599: 316.358398\n",
      "loss at batch 180799: 320.190979\n",
      "loss at batch 180999: 321.775452\n",
      "loss at batch 181199: 308.452606\n",
      "loss at batch 181399: 316.855225\n",
      "loss at batch 181599: 324.895935\n",
      "loss at batch 181799: 312.272644\n",
      "loss at batch 181999: 313.393463\n",
      "loss at batch 182199: 331.760101\n",
      "loss at batch 182399: 317.302979\n",
      "loss at batch 182599: 306.515686\n",
      "loss at batch 182799: 304.646667\n",
      "loss at batch 182999: 320.702148\n",
      "loss at batch 183199: 306.249786\n",
      "loss at batch 183399: 318.140991\n",
      "loss at batch 183599: 328.823517\n",
      "loss at batch 183799: 317.128479\n",
      "loss at batch 183999: 310.564545\n",
      "loss at batch 184199: 307.783813\n",
      "loss at batch 184399: 315.200317\n",
      "loss at batch 184599: 309.706238\n",
      "loss at batch 184799: 307.511108\n",
      "-------------------\n",
      "Batch: 184999, train time: 429.534 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5827851682366048, 'mAUC': 0.9882315048003514, 'd_prime': 3.2026337096290036}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34087208601126384, 'mAUC': 0.9617061007205019, 'd_prime': 2.5043418094698713}\n",
      "-------------------\n",
      "\n",
      "loss at batch 184999: 315.012329\n",
      "loss at batch 185199: 320.163757\n",
      "loss at batch 185399: 317.044189\n",
      "loss at batch 185599: 333.762573\n",
      "loss at batch 185799: 313.070129\n",
      "loss at batch 185999: 310.940674\n",
      "loss at batch 186199: 337.204651\n",
      "loss at batch 186399: 313.776398\n",
      "loss at batch 186599: 320.340454\n",
      "loss at batch 186799: 321.927307\n",
      "loss at batch 186999: 318.348907\n",
      "loss at batch 187199: 316.882690\n",
      "loss at batch 187399: 321.243347\n",
      "loss at batch 187599: 309.659668\n",
      "loss at batch 187799: 315.656677\n",
      "loss at batch 187999: 318.097778\n",
      "loss at batch 188199: 321.412445\n",
      "loss at batch 188399: 324.276642\n",
      "loss at batch 188599: 303.300354\n",
      "loss at batch 188799: 328.171661\n",
      "loss at batch 188999: 327.340210\n",
      "loss at batch 189199: 303.717346\n",
      "loss at batch 189399: 314.308167\n",
      "loss at batch 189599: 308.910461\n",
      "loss at batch 189799: 318.084656\n",
      "-------------------\n",
      "Batch: 189999, train time: 431.999 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.583481713215473, 'mAUC': 0.9883016298187487, 'd_prime': 3.205871347899069}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34118916956400863, 'mAUC': 0.9617521791713105, 'd_prime': 2.505125708554304}\n",
      "-------------------\n",
      "\n",
      "loss at batch 189999: 322.861420\n",
      "loss at batch 190199: 319.684479\n",
      "loss at batch 190399: 326.392548\n",
      "loss at batch 190599: 333.088867\n",
      "loss at batch 190799: 315.682983\n",
      "loss at batch 190999: 319.309814\n",
      "loss at batch 191199: 323.483643\n",
      "loss at batch 191399: 315.841858\n",
      "loss at batch 191599: 327.831329\n",
      "loss at batch 191799: 310.318298\n",
      "loss at batch 191999: 307.586121\n",
      "loss at batch 192199: 320.654663\n",
      "loss at batch 192399: 314.564819\n",
      "loss at batch 192599: 313.680054\n",
      "loss at batch 192799: 322.542480\n",
      "loss at batch 192999: 314.655396\n",
      "loss at batch 193199: 319.175232\n",
      "loss at batch 193399: 318.686279\n",
      "loss at batch 193599: 327.650635\n",
      "loss at batch 193799: 319.226562\n",
      "loss at batch 193999: 302.920929\n",
      "loss at batch 194199: 322.703552\n",
      "loss at batch 194399: 314.514313\n",
      "loss at batch 194599: 304.475220\n",
      "loss at batch 194799: 319.963379\n",
      "-------------------\n",
      "Batch: 194999, train time: 431.566 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5838454284971571, 'mAUC': 0.988328152519446, 'd_prime': 3.2071002799356827}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3412392681247961, 'mAUC': 0.9617853616271305, 'd_prime': 2.505690694564692}\n",
      "-------------------\n",
      "\n",
      "loss at batch 194999: 313.363403\n",
      "loss at batch 195199: 314.081726\n",
      "loss at batch 195399: 316.952637\n",
      "loss at batch 195599: 313.907745\n",
      "loss at batch 195799: 320.784882\n",
      "loss at batch 195999: 321.398315\n",
      "loss at batch 196199: 315.322388\n",
      "loss at batch 196399: 327.952454\n",
      "loss at batch 196599: 309.146851\n",
      "loss at batch 196799: 304.398346\n",
      "loss at batch 196999: 312.888794\n",
      "loss at batch 197199: 321.520721\n",
      "loss at batch 197399: 317.877350\n",
      "loss at batch 197599: 315.801605\n",
      "loss at batch 197799: 315.569550\n",
      "loss at batch 197999: 304.840454\n",
      "loss at batch 198199: 322.011230\n",
      "loss at batch 198399: 322.626160\n",
      "loss at batch 198599: 311.576355\n",
      "loss at batch 198799: 321.287170\n",
      "loss at batch 198999: 323.464783\n",
      "loss at batch 199199: 314.442413\n",
      "loss at batch 199399: 315.820068\n",
      "loss at batch 199599: 330.988220\n",
      "loss at batch 199799: 303.778412\n",
      "-------------------\n",
      "Batch: 199999, train time: 429.718 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5839981510142659, 'mAUC': 0.988345387236752, 'd_prime': 3.207900152383663}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34150642062493736, 'mAUC': 0.9618521603954298, 'd_prime': 2.506829268108952}\n",
      "-------------------\n",
      "\n",
      "loss at batch 199999: 313.280273\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optimization method\n",
    "optimizer = Adam(lr=lr)\n",
    "model.compile(loss=binary_focal_loss(alpha=.25, gamma=2), optimizer=optimizer)\n",
    "# model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "DataGenerator = BalGen\n",
    "\n",
    "train_gen = DataGenerator(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "train_time = time.time()\n",
    "\n",
    "for (batch_x, batch_y) in train_gen.generate():\n",
    "\n",
    "    # Compute stats every several interations\n",
    "    if i_batch % eval_per_n_batches == eval_per_n_batches-1:\n",
    "\n",
    "        print(\"-------------------\")\n",
    "\n",
    "        print(f\"Batch: {i_batch}, train time: {time.time() - train_time:.3f} s\")\n",
    "\n",
    "        bal_metrics = evaluate(\n",
    "            model=model,\n",
    "            x=train_bal_x,\n",
    "            target=train_bal_y,\n",
    "        )\n",
    "        bal_metrics_list.append(bal_metrics)\n",
    "        print(\"Balanced train metrics:\")\n",
    "        print(bal_metrics)\n",
    "        \n",
    "        test_metrics = evaluate(\n",
    "            model=model,\n",
    "            x=test_x,\n",
    "            target=test_y,\n",
    "        )\n",
    "        test_metrics_list.append(test_metrics)\n",
    "        print(\"Test train metrics:\")\n",
    "        print(test_metrics)\n",
    "        \n",
    "        print(\"-------------------\\n\")\n",
    "        \n",
    "        train_time = time.time()\n",
    "\n",
    "    # Update params\n",
    "#     (batch_x, batch_y) = transform_data(batch_x, batch_y)\n",
    "    loss_i = model.train_on_batch(x=batch_x, y=batch_y)\n",
    "    if i_batch % print_loss_every_n_batches == print_loss_every_n_batches-1:\n",
    "        loss_curve_list.append({'batch': i_batch, 'loss': loss_i})\n",
    "        print(f'loss at batch {i_batch}: {loss_i:.6f}')\n",
    "    \n",
    "    # Save model\n",
    "    if i_batch % save_per_n_batches == save_per_n_batches-1:\n",
    "        save_model_path = model_path/(f'{model_name}after_{i_batch+1:06d}batches.h5')\n",
    "        model.save(save_model_path)\n",
    "        \n",
    "    i_batch += 1\n",
    "\n",
    "    # Stop training when maximum iteration achieves\n",
    "    if i_batch >= train_n_batches:\n",
    "#         model.save(model_path/(f'{model_name}_final.h5'))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 5 with Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "model_name = 'attn_feat_02_layers-2-stage_05-focal_loss-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n_batches += 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch 200199: 325.848480\n",
      "loss at batch 200399: 305.104431\n",
      "loss at batch 200599: 322.077271\n",
      "loss at batch 200799: 322.055054\n",
      "loss at batch 200999: 338.385742\n",
      "loss at batch 201199: 319.056976\n",
      "loss at batch 201399: 324.965271\n",
      "loss at batch 201599: 311.024658\n",
      "loss at batch 201799: 314.195435\n",
      "loss at batch 201999: 326.421631\n",
      "loss at batch 202199: 317.525452\n",
      "loss at batch 202399: 335.346741\n",
      "loss at batch 202599: 325.911743\n",
      "loss at batch 202799: 316.437622\n",
      "loss at batch 202999: 304.122284\n",
      "loss at batch 203199: 322.286865\n",
      "loss at batch 203399: 324.979645\n",
      "loss at batch 203599: 302.226868\n",
      "loss at batch 203799: 317.030884\n",
      "loss at batch 203999: 317.640625\n",
      "loss at batch 204199: 324.331329\n",
      "loss at batch 204399: 307.933167\n",
      "loss at batch 204599: 323.790192\n",
      "loss at batch 204799: 316.023682\n",
      "-------------------\n",
      "Batch: 204999, train time: 443.804 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5846475326544633, 'mAUC': 0.9883621264175607, 'd_prime': 3.2086780103199266}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34169749676302164, 'mAUC': 0.9618798757340892, 'd_prime': 2.5073021486176064}\n",
      "-------------------\n",
      "\n",
      "loss at batch 204999: 314.632996\n",
      "loss at batch 205199: 321.525085\n",
      "loss at batch 205399: 301.923889\n",
      "loss at batch 205599: 312.585541\n",
      "loss at batch 205799: 321.124878\n",
      "loss at batch 205999: 325.418762\n",
      "loss at batch 206199: 331.336731\n",
      "loss at batch 206399: 312.477509\n",
      "loss at batch 206599: 328.135315\n",
      "loss at batch 206799: 322.373444\n",
      "loss at batch 206999: 307.507690\n",
      "loss at batch 207199: 318.379822\n",
      "loss at batch 207399: 313.341583\n",
      "loss at batch 207599: 316.212219\n",
      "loss at batch 207799: 321.810791\n",
      "loss at batch 207999: 315.533051\n",
      "loss at batch 208199: 325.231445\n",
      "loss at batch 208399: 306.545715\n",
      "loss at batch 208599: 318.980469\n",
      "loss at batch 208799: 328.953888\n",
      "loss at batch 208999: 317.734619\n",
      "loss at batch 209199: 310.070068\n",
      "loss at batch 209399: 311.859222\n",
      "loss at batch 209599: 314.325836\n",
      "loss at batch 209799: 321.816101\n",
      "-------------------\n",
      "Batch: 209999, train time: 429.614 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5848205386199385, 'mAUC': 0.9883748435195782, 'd_prime': 3.2092696146373365}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3416973370142398, 'mAUC': 0.9618851714298877, 'd_prime': 2.507392535954047}\n",
      "-------------------\n",
      "\n",
      "loss at batch 209999: 303.281067\n",
      "loss at batch 210199: 318.304443\n",
      "loss at batch 210399: 306.657898\n",
      "loss at batch 210599: 313.283264\n",
      "loss at batch 210799: 311.876099\n",
      "loss at batch 210999: 303.289154\n",
      "loss at batch 211199: 316.816772\n",
      "loss at batch 211399: 320.972473\n",
      "loss at batch 211599: 315.585999\n",
      "loss at batch 211799: 319.135834\n",
      "loss at batch 211999: 316.130768\n",
      "loss at batch 212199: 324.853760\n",
      "loss at batch 212399: 312.111176\n",
      "loss at batch 212599: 302.249786\n",
      "loss at batch 212799: 302.117889\n",
      "loss at batch 212999: 320.569702\n",
      "loss at batch 213199: 321.442200\n",
      "loss at batch 213399: 314.807648\n",
      "loss at batch 213599: 316.082916\n",
      "loss at batch 213799: 318.276062\n",
      "loss at batch 213999: 315.479156\n",
      "loss at batch 214199: 316.492310\n",
      "loss at batch 214399: 302.477325\n",
      "loss at batch 214599: 322.551453\n",
      "loss at batch 214799: 315.560364\n",
      "-------------------\n",
      "Batch: 214999, train time: 430.796 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5850017494216024, 'mAUC': 0.9883874219487938, 'd_prime': 3.20985532082772}\n",
      "Test train metrics:\n",
      "{'mAP': 0.341812979636182, 'mAUC': 0.9619076628760923, 'd_prime': 2.5077765358325945}\n",
      "-------------------\n",
      "\n",
      "loss at batch 214999: 319.432068\n",
      "loss at batch 215199: 323.936340\n",
      "loss at batch 215399: 326.980652\n",
      "loss at batch 215599: 309.308228\n",
      "loss at batch 215799: 327.224609\n",
      "loss at batch 215999: 323.431549\n",
      "loss at batch 216199: 326.631378\n",
      "loss at batch 216399: 327.612183\n",
      "loss at batch 216599: 347.115845\n",
      "loss at batch 216799: 336.418701\n",
      "loss at batch 216999: 326.546570\n",
      "loss at batch 217199: 325.086639\n",
      "loss at batch 217399: 297.412415\n",
      "loss at batch 217599: 323.255615\n",
      "loss at batch 217799: 339.252075\n",
      "loss at batch 217999: 311.635193\n",
      "loss at batch 218199: 317.550842\n",
      "loss at batch 218399: 325.956635\n",
      "loss at batch 218599: 319.699463\n",
      "loss at batch 218799: 319.712189\n",
      "loss at batch 218999: 318.328217\n",
      "loss at batch 219199: 315.250305\n",
      "loss at batch 219399: 308.056732\n",
      "loss at batch 219599: 325.758270\n",
      "loss at batch 219799: 314.675598\n",
      "-------------------\n",
      "Batch: 219999, train time: 430.159 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5853890650114753, 'mAUC': 0.988399971576194, 'd_prime': 3.2104402350798136}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3417516595808455, 'mAUC': 0.9619058559483535, 'd_prime': 2.5077456790498687}\n",
      "-------------------\n",
      "\n",
      "loss at batch 219999: 309.915710\n",
      "loss at batch 220199: 312.897888\n",
      "loss at batch 220399: 316.375397\n",
      "loss at batch 220599: 320.728516\n",
      "loss at batch 220799: 320.746887\n",
      "loss at batch 220999: 317.439148\n",
      "loss at batch 221199: 324.169037\n",
      "loss at batch 221399: 304.979797\n",
      "loss at batch 221599: 317.903351\n",
      "loss at batch 221799: 321.640228\n",
      "loss at batch 221999: 306.476196\n",
      "loss at batch 222199: 309.986176\n",
      "loss at batch 222399: 312.415222\n",
      "loss at batch 222599: 310.018433\n",
      "loss at batch 222799: 320.811218\n",
      "loss at batch 222999: 316.011200\n",
      "loss at batch 223199: 320.484375\n",
      "loss at batch 223399: 309.585846\n",
      "loss at batch 223599: 316.999420\n",
      "loss at batch 223799: 321.575317\n",
      "loss at batch 223999: 315.009399\n",
      "loss at batch 224199: 306.379822\n",
      "loss at batch 224399: 309.890717\n",
      "loss at batch 224599: 321.485016\n",
      "loss at batch 224799: 327.497864\n",
      "-------------------\n",
      "Batch: 224999, train time: 430.321 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5853346394739751, 'mAUC': 0.9884045433065353, 'd_prime': 3.210653451272764}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3418257405624844, 'mAUC': 0.9619102549909532, 'd_prime': 2.507820803287564}\n",
      "-------------------\n",
      "\n",
      "loss at batch 224999: 323.897644\n",
      "loss at batch 225199: 319.500885\n",
      "loss at batch 225399: 313.151154\n",
      "loss at batch 225599: 323.437866\n",
      "loss at batch 225799: 294.710815\n",
      "loss at batch 225999: 321.262482\n",
      "loss at batch 226199: 323.999451\n",
      "loss at batch 226399: 302.120453\n",
      "loss at batch 226599: 307.984253\n",
      "loss at batch 226799: 307.964050\n",
      "loss at batch 226999: 310.155060\n",
      "loss at batch 227199: 319.412201\n",
      "loss at batch 227399: 324.326935\n",
      "loss at batch 227599: 311.016815\n",
      "loss at batch 227799: 311.613953\n",
      "loss at batch 227999: 322.248230\n",
      "loss at batch 228199: 312.228943\n",
      "loss at batch 228399: 327.752502\n",
      "loss at batch 228599: 333.682587\n",
      "loss at batch 228799: 311.850037\n",
      "loss at batch 228999: 297.702881\n",
      "loss at batch 229199: 332.383118\n",
      "loss at batch 229399: 314.556183\n",
      "loss at batch 229599: 308.234070\n",
      "loss at batch 229799: 326.944672\n",
      "-------------------\n",
      "Batch: 229999, train time: 429.980 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5853366103315468, 'mAUC': 0.9884045984559249, 'd_prime': 3.2106560237735216}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3416347614865958, 'mAUC': 0.9619036753216221, 'd_prime': 2.50770844222442}\n",
      "-------------------\n",
      "\n",
      "loss at batch 229999: 328.701752\n",
      "loss at batch 230199: 320.246307\n",
      "loss at batch 230399: 321.246613\n",
      "loss at batch 230599: 314.001343\n",
      "loss at batch 230799: 310.586853\n",
      "loss at batch 230999: 310.930725\n",
      "loss at batch 231199: 307.181213\n",
      "loss at batch 231399: 315.692017\n",
      "loss at batch 231599: 324.647034\n",
      "loss at batch 231799: 315.509857\n",
      "loss at batch 231999: 310.900879\n",
      "loss at batch 232199: 325.493958\n",
      "loss at batch 232399: 319.398743\n",
      "loss at batch 232599: 304.614075\n",
      "loss at batch 232799: 300.125763\n",
      "loss at batch 232999: 319.363739\n",
      "loss at batch 233199: 297.013306\n",
      "loss at batch 233399: 315.073853\n",
      "loss at batch 233599: 315.942993\n",
      "loss at batch 233799: 309.658264\n",
      "loss at batch 233999: 317.215454\n",
      "loss at batch 234199: 310.654236\n",
      "loss at batch 234399: 321.487396\n",
      "loss at batch 234599: 303.407898\n",
      "loss at batch 234799: 307.836426\n",
      "-------------------\n",
      "Batch: 234999, train time: 430.293 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5856261915546713, 'mAUC': 0.9884131709462189, 'd_prime': 3.2110560257489666}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34160328203963075, 'mAUC': 0.961908735399034, 'd_prime': 2.5077948517989412}\n",
      "-------------------\n",
      "\n",
      "loss at batch 234999: 319.752380\n",
      "loss at batch 235199: 327.346100\n",
      "loss at batch 235399: 314.967346\n",
      "loss at batch 235599: 327.436554\n",
      "loss at batch 235799: 309.661499\n",
      "loss at batch 235999: 309.145416\n",
      "loss at batch 236199: 331.549622\n",
      "loss at batch 236399: 315.345337\n",
      "loss at batch 236599: 323.918060\n",
      "loss at batch 236799: 315.792358\n",
      "loss at batch 236999: 316.869781\n",
      "loss at batch 237199: 319.631348\n",
      "loss at batch 237399: 326.518982\n",
      "loss at batch 237599: 308.661987\n",
      "loss at batch 237799: 317.515564\n",
      "loss at batch 237999: 315.824341\n",
      "loss at batch 238199: 323.023468\n",
      "loss at batch 238399: 323.807892\n",
      "loss at batch 238599: 313.104126\n",
      "loss at batch 238799: 315.374847\n",
      "loss at batch 238999: 321.482117\n",
      "loss at batch 239199: 301.847992\n",
      "loss at batch 239399: 312.339050\n",
      "loss at batch 239599: 312.134552\n",
      "loss at batch 239799: 313.839447\n",
      "-------------------\n",
      "Batch: 239999, train time: 429.850 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.585712710482629, 'mAUC': 0.9884211553427024, 'd_prime': 3.2114288178080455}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34170123609959235, 'mAUC': 0.9619080298237065, 'd_prime': 2.5077828023180166}\n",
      "-------------------\n",
      "\n",
      "loss at batch 239999: 318.013916\n",
      "loss at batch 240199: 320.562622\n",
      "loss at batch 240399: 318.938629\n",
      "loss at batch 240599: 324.511963\n",
      "loss at batch 240799: 317.292786\n",
      "loss at batch 240999: 314.259705\n",
      "loss at batch 241199: 322.566193\n",
      "loss at batch 241399: 315.935486\n",
      "loss at batch 241599: 332.195923\n",
      "loss at batch 241799: 309.659119\n",
      "loss at batch 241999: 304.401886\n",
      "loss at batch 242199: 315.039429\n",
      "loss at batch 242399: 318.748047\n",
      "loss at batch 242599: 316.129395\n",
      "loss at batch 242799: 325.620178\n",
      "loss at batch 242999: 313.927673\n",
      "loss at batch 243199: 315.090393\n",
      "loss at batch 243399: 312.935547\n",
      "loss at batch 243599: 325.419403\n",
      "loss at batch 243799: 310.971863\n",
      "loss at batch 243999: 302.174316\n",
      "loss at batch 244199: 315.038971\n",
      "loss at batch 244399: 322.735718\n",
      "loss at batch 244599: 307.180176\n",
      "loss at batch 244799: 318.610321\n",
      "-------------------\n",
      "Batch: 244999, train time: 437.507 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5857741799342385, 'mAUC': 0.9884195879319262, 'd_prime': 3.2113556176787093}\n",
      "Test train metrics:\n",
      "{'mAP': 0.34170861843803946, 'mAUC': 0.9619160814646481, 'd_prime': 2.507920315240887}\n",
      "-------------------\n",
      "\n",
      "loss at batch 244999: 311.833008\n",
      "loss at batch 245199: 318.214539\n",
      "loss at batch 245399: 316.967743\n",
      "loss at batch 245599: 307.457794\n",
      "loss at batch 245799: 324.228638\n",
      "loss at batch 245999: 321.873047\n",
      "loss at batch 246199: 317.075256\n",
      "loss at batch 246399: 328.896301\n",
      "loss at batch 246599: 312.733887\n",
      "loss at batch 246799: 301.813477\n",
      "loss at batch 246999: 311.792145\n",
      "loss at batch 247199: 321.593994\n",
      "loss at batch 247399: 322.658447\n",
      "loss at batch 247599: 311.725189\n",
      "loss at batch 247799: 314.344269\n",
      "loss at batch 247999: 310.346130\n",
      "loss at batch 248199: 321.202393\n",
      "loss at batch 248399: 318.547791\n",
      "loss at batch 248599: 316.458344\n",
      "loss at batch 248799: 320.210815\n",
      "loss at batch 248999: 320.712402\n",
      "loss at batch 249199: 308.472595\n",
      "loss at batch 249399: 312.256104\n",
      "loss at batch 249599: 333.500671\n",
      "loss at batch 249799: 314.971802\n",
      "-------------------\n",
      "Batch: 249999, train time: 433.086 s\n",
      "Balanced train metrics:\n",
      "{'mAP': 0.5858772386706693, 'mAUC': 0.9884309218828538, 'd_prime': 3.2118851219211644}\n",
      "Test train metrics:\n",
      "{'mAP': 0.3418220227097377, 'mAUC': 0.96192591887088, 'd_prime': 2.50808835920614}\n",
      "-------------------\n",
      "\n",
      "loss at batch 249999: 312.792175\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optimization method\n",
    "optimizer = Adam(lr=lr)\n",
    "model.compile(loss=binary_focal_loss(alpha=.25, gamma=2), optimizer=optimizer)\n",
    "# model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "DataGenerator = BalGen\n",
    "\n",
    "train_gen = DataGenerator(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "train_time = time.time()\n",
    "\n",
    "for (batch_x, batch_y) in train_gen.generate():\n",
    "\n",
    "    # Compute stats every several interations\n",
    "    if i_batch % eval_per_n_batches == eval_per_n_batches-1:\n",
    "\n",
    "        print(\"-------------------\")\n",
    "\n",
    "        print(f\"Batch: {i_batch}, train time: {time.time() - train_time:.3f} s\")\n",
    "\n",
    "        bal_metrics = evaluate(\n",
    "            model=model,\n",
    "            x=train_bal_x,\n",
    "            target=train_bal_y,\n",
    "        )\n",
    "        bal_metrics_list.append(bal_metrics)\n",
    "        print(\"Balanced train metrics:\")\n",
    "        print(bal_metrics)\n",
    "        \n",
    "        test_metrics = evaluate(\n",
    "            model=model,\n",
    "            x=test_x,\n",
    "            target=test_y,\n",
    "        )\n",
    "        test_metrics_list.append(test_metrics)\n",
    "        print(\"Test train metrics:\")\n",
    "        print(test_metrics)\n",
    "        \n",
    "        print(\"-------------------\\n\")\n",
    "        \n",
    "        train_time = time.time()\n",
    "\n",
    "    # Update params\n",
    "#     (batch_x, batch_y) = transform_data(batch_x, batch_y)\n",
    "    loss_i = model.train_on_batch(x=batch_x, y=batch_y)\n",
    "    if i_batch % print_loss_every_n_batches == print_loss_every_n_batches-1:\n",
    "        loss_curve_list.append({'batch': i_batch, 'loss': loss_i})\n",
    "        print(f'loss at batch {i_batch}: {loss_i:.6f}')\n",
    "    \n",
    "    # Save model\n",
    "    if i_batch % save_per_n_batches == save_per_n_batches-1:\n",
    "        save_model_path = model_path/(f'{model_name}after_{i_batch+1:06d}batches.h5')\n",
    "        model.save(save_model_path)\n",
    "        \n",
    "    i_batch += 1\n",
    "\n",
    "    # Stop training when maximum iteration achieves\n",
    "    if i_batch >= train_n_batches:\n",
    "#         model.save(model_path/(f'{model_name}_final.h5'))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
